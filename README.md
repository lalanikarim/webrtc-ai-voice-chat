<a href="https://github.com/you"><img decoding="async" width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_right_green_007200.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" loading="lazy" data-recalc-dims="1"></a>
WebRTC AI Voice Chat
====================

Overview
--------

The goal of this project is to demo `speech <-> langchain <-> audio` workflow.

1. Speech to text is using [OpenAI's open source Whisper mini](https://huggingface.co/openai/whisper-small) model.
2. Chat model used for this demo is [Microsoft's Phi3](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) model running locally using [Ollama](https://ollama.com/).
3. Text to Audio is using [Suno's open source Bark small](https://huggingface.co/suno/bark-small) model.

For interesting projects and related resources, checkout the [Awesome Projects Page](AwesomeProjects.md).

Demo
----

https://github.com/lalanikarim/webrtc-ai-voice-chat/assets/1296705/22e31a0b-bc59-44ca-b7a3-2370648f193e

